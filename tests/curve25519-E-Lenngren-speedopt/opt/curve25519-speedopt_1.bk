// Copyright 2017-2025 Emil Lenngren

// This is an armv7 implementation of X25519.
// It follows the reference implementation where the representation of
// a field element [0..2^255-19) is represented by a 256-bit little endian integer,
// reduced modulo 2^256-38.
// The scalar is a 256-bit integer where certain bits are hardcoded per specification.
//
// The implementation runs in constant time (measured 428 283 cycles on ARM Cortex-M4F,
// few wait states), and no conditional branches depend on secret data.
//
// This implementation conditionally loads data from different RAM locations depending
// on secret data, so this version should not be used on CPUs that have data cache,
// such as Cortex-A53, unless the has_d_cache define is set. It's suited for embedded
// devices running CPUs like Cortex-M4 and Cortex-M33.

//#define has_d_cache

#define START_FUNC(name) \
	.thumb_func ;\
	.type name, %function ;\
name: ;\
	.cfi_startproc
#define START_GLOBAL_FUNC(name) \
	.global name ;\
	START_FUNC(name)
#define END_FUNC(name) \
	.cfi_endproc ;\
	.size name, .-name

#if defined(__ARM_FP) && defined(__ARM_ARCH_7EM__)
#define has_fpu
#else
#error "NO FPU"
#endif

	.syntax unified
	.text
	.eabi_attribute Tag_ABI_align_preserved, 1 // only internal functions called, so ok
	.cfi_sections .debug_frame
	.thumb
	.balign 4

// input: *r1=a, *r2=b
// output: r4-r11
// clobbers all other registers
// cycles: 39
START_FUNC(fe25519_add)
	ldr r3,[r1,#28]
	ldr r4,[r2,#28]
	movs r0,#1
	movs r6,#2
	mov.w r5,r3
	umaal r3,r5,r6,r4
	movs r7,#19
	mul r11,r7,r5
	
	ldm r1!,{r4-r7}
	ldm r2!,{r8-r10,r12}
	umaal r4,r11,r0,r8
	umaal r5,r11,r0,r9
	umaal r6,r11,r0,r10
	umaal r7,r11,r0,r12
	ldm r1,{r8-r10}
	ldm r2,{r1,r2,r12}
	umaal r8,r11,r0,r1
	umaal r9,r11,r0,r2
	umaal r10,r11,r0,r12
	add r11,r11,r3, lsr #1
	bx lr
END_FUNC(fe25519_add)

	.balign 4
// input: *r1=a, *r2=b
// output: r4-r11
// clobbers all other registers
// cycles: 41
// note: works only when a - b >= -(2^256-38)
START_FUNC(fe25519_sub)
	ldm r1,{r4-r11}
	ldm r2!,{r0,r1,r3,r12}
	subs r4,r4,r0
	sbcs r5,r5,r1
	sbcs r6,r6,r3
	sbcs r7,r7,r12
	ldm r2,{r0-r3}
	sbcs r8,r8,r0
	sbcs r9,r9,r1
	sbcs r10,r10,r2
	sbcs r11,r11,r3
	
	// if subtraction goes below 0, set r0 to -1 and r1 to -38, else set both to 0
	sbcs r0,r0,r0
	and r1,r0,#-38
	
	adds r4,r4,r1
	adcs r5,r5,r0
	adcs r6,r6,r0
	adcs r7,r7,r0
	adcs r8,r8,r0
	adcs r9,r9,r0
	adcs r10,r10,r0
	adcs r11,r11,r0
	
	// if it is desired that all 256-bit inputs shall work, uncomment the following lines:
	//adcs r0,r0,#0
	//and r1,r0,#-38
	//add r4,r4,r1
	
	bx lr
END_FUNC(fe25519_sub)
	
#ifdef has_fpu
// input: *r1=a, *r2=b
// output: r4-r11
// clobbers all other registers
// cycles: 140
START_FUNC(fe25519_mul)
	push {lr}
	.cfi_def_cfa_offset 4
	.cfi_offset lr,-4
	
	vmov s6,r1
	ldm r1!,{r9}
	vldm r2,{s8-s15}
	ldm r1,{r0,r1,r2,r3,r4} // kill r1
	vmov r5,r6,s8,s9
	vmov r10,s10
start_label:
	umull lr,r8,r5,r9 // A=lr done
	umull r7,r11,r5,r0
	// 11 8+7 lr
	umaal r7,r8,r6,r9 // B=r7 done
	// 11+8 7 lr
	vmov s0,s1,lr,r7
	umull r7,r12,r5,r1
	// 12 11+8+7 . .
	umaal r7,r11,r6,r0
	// 12+11 8+7 . .
	umaal r7,r8,r10,r9 // C=r7 done
	// 12+11+8 7 . .
	umaal r11,r12,r5,r2
	vmov s2,r7
	// 12 11+8 . . .
	umaal r8,r11,r6,r1
	// 12+11 8 . . .
	umaal r11,r12,r5,r3
	// 12 11 8 . . .
	umull r7,lr,r10,r0
	// 12 lr+11 8+7 . . .
	umaal r11,lr,r6,r2
	// lr+12 11 8+7 . . .
	umaal r12,lr,r6,r3
	// lr 12 11 8+7 . . .
	vmov r6,s11
	umaal r7,r8,r6,r9 // r7 done
	// lr 12 11+8 7 . . .
	vmov s3,r7
	umaal r8,r11,r10,r1
	// lr 12+11 8 . . . .
	umaal r11,r12,r10,r2
	// lr+12 11 8 . . . .
	umaal r12,lr,r10,r3
	// lr 12 11 8 . . . .
	umull r7,r10,r6,r0
	// lr 12 11+10 8+7 . . . .
	umaal r10,r11,r6,r1
	umaal r11,r12,r6,r2
	umaal r12,lr,r6,r3


	// lr 12 11 10 8+7 . . . .

	vmov r6,s12
	umaal r7,r8,r6,r9 // r7 done
	vmov s4,r7
	umaal r8,r10,r6,r0
	umaal r10,r11,r6,r1
	umaal r11,r12,r6,r2
	umaal r12,lr,r6,r3

	// lr 12 11 10 8

	vmov r6,s13
	umull r7,r5,r5,r4

	// lr 12 11 10+5 8+7

	umaal r7,r8,r6,r9
	umaal r5,r10,r6,r0
	umaal r10,r11,r6,r1
	umaal r11,r12,r6,r2
	umaal r12,lr,r6,r3

	// lr 12 11 10 8+5
	vmov s5,r7

	vmov r7,r6,s14,s15
	umaal r5,r8,r7,r9
	umaal r8,r10,r7,r0
	umaal r10,r11,r7,r1
	umaal r11,r12,r7,r2
	umaal r12,lr,r7,r3

	// lr 12 11 10 8 5

	umull r7,r9,r6,r9
	umaal r9,r10,r6,r0
	umaal r10,r11,r6,r1
	umaal r11,r12,r6,r2
	umaal r12,lr,r6,r3

	// lr 12 11 10 9 8+7 5


	vmov r0,s6
	vmov r1,r3,s8,s9
	ldr r4,[r0,#4*6]
	ldr r2,[r0,#4*7]
	ldr.w r0,[r0,#4*5]
	umaal r7,r8,r1,r2
	umull r6,r1,r1,r4

	// lr 12 11 10 9+8 7+4 6+5


	umaal r6,r5,r3,r0
	vmov s6,r6
	umaal r1,r7,r3,r4
	umaal r8,r9,r3,r2

	vmov r3,r6,s10,s11
	umaal r5,r1,r3,r0
	umaal r7,r8,r3,r4
	umaal r9,r10,r3,r2

	umaal r1,r7,r6,r0
	umaal r8,r9,r6,r4
	umaal r10,r11,r6,r2

	vmov r3,r6,s12,s13
	umaal r7,r8,r3,r0
	umaal r9,r10,r3,r4
	umaal r11,r12,r3,r2

	umaal r8,r9,r6,r0
	umaal r10,r11,r6,r4
	umaal r12,lr,r6,r2

	vmov r3,r6,s14,s15
	umaal r9,r10,r3,r0
	umaal r11,r12,r3,r4
	umaal r12,lr,r3,r2

	umaal r10,r11,r6,r0
	umaal r12,r11,r6,r4
	umaal lr,r11,r6,r2

	// 11 lr 12 10 9 8 7 1 5 fpu*7


	//now reduce


	movs r0,#38<<1
	mov r3,r5
	umaal r3,r5,r0,r11
	movs r0,#19
	mov r11,#38
	mul r0,r5,r0
	
	vmov r4,r5,s0,s1
	umaal r4,r0,r11,r1
	umaal r5,r0,r11,r7
	vmov r6,r7,s2,s3
	umaal r6,r0,r11,r8
	umaal r7,r0,r11,r9
	vmov r8,r9,s4,s5
	umaal r8,r0,r11,r10
	umaal r9,r0,r11,r12
	vmov r10,s6
	umaal r10,r0,r11,lr
	add r11,r0,r3, lsr #1
end_label:

	pop {pc}
END_FUNC(fe25519_mul)

	.balign 4
// input/result in (r4-r11)
// clobbers all other registers
// cycles: 88
START_FUNC(fe25519_sqr)
	str lr,[sp,#-4]!
	.cfi_def_cfa_offset 4
	.cfi_offset lr,-4
fe25519_sqr_nopush:
	
#define q0 r4
#define q1 r5
#define q2 r6
#define q3 r7
#define q4 r8
#define q5 r9
#define q6 r10
#define q7 r11
	movs r0,#0

	umull r1,r2,q0,q0
	and r12,q1,q0, asr #31
	adds q0,q0,q0
	vmov s0,r1
	umull r1,r3,r0,r0 // set to 0
	umaal r1,r2,q0,q1
	umaal r2,r12,q0,q2
	umaal r2,r3,q1,q1
	vmov s1,s2,r1,r2
	and lr,q2,q1, asr #31
	adcs.w q1,q1,q1
	umull r1,r2,r0,r0 // set to 0
	umaal r3,r12,q0,q3
	umaal r1,r3,q1,q2
	umaal r3,r12,q0,q4
	umaal r3,lr,q1,q3
	umaal r2,r3,q2,q2
	vmov s3,s4,r1,r2
	umaal r3,r12,q0,q5
	umaal r3,lr,q1,q4
	umaal r12,lr,q0,q6
	and r2,q3,q2, asr #31
	adcs q2,q2,q2
	umaal r2,r12,q1,q5
	umaal r12,lr,q0,q7 // q0 dead
	umull r1,r4,r0,r0 // set to 0
	umaal r1,r3,q2,q3               // r1 (r9) done
	umaal r2,r3,q2,q4
	umaal r3,r12,q1,q6
	umaal r12,lr,q1,q7 // q1 dead
	umaal r2,r4,q3,q3               // r2 done
	umaal r0,r3,q2,q5
	umaal r3,r12,q2,q6
	umaal r12,lr,q2,q7 // q2 dead
	and r6,q4,q3, asr #31
	adcs q3,q3,q3
	umaal r4,r0,q3,q4               // r4 done
	umaal r0,r3,q3,q5
	umaal r3,r12,q3,q6
	umaal r12,lr,q3,q7 // q3 dead
	umaal r6,r0,q4,q4               // r6 done
	adcs r5,q4,q4
	umaal r0,r3,r5,q5               // r0 done
	umaal r3,r12,r5,q6
	umaal r12,lr,r5,q7 // r5 dead
	and r8,q5,q4, asr #31
	umaal r8,r3,q5,q5               // r8 done
	and r5,q6,q5, asr #31
	adcs q5,q5,q5
	umaal r3,r12,q5,q6              // r3 done
	umaal r12,lr,q5,q7 // q5 dead
	umaal r12,r5,q6,q6              // r12 done
	mov.w r9,r1
	and r1,q7,q6, asr #31
	adcs q6,q6,q6
	umaal lr,r5,q6,q7               // lr done
	umaal r1,r5,q7,q7               // r1, r5 done

	// msb -> lsb: r5 r1 lr r12 r3 r8 r0 r6 r4 r2 r9 sp[4] sp[3] sp[2] sp[1] sp[0]
	// now reduce!

	movs r7,#38*2
	mov r11,r4
	umaal r11,r4,r7,r5
	movs r5,#19
	mov r7,r0
	movs r0,#38
	muls r4,r5,r4

	vmov r5,s0
	umaal r4,r5,r0,r6
	vmov r6,r10,s1,s2
	umaal r5,r6,r0,r7
	umaal r6,r10,r0,r8
	vmov r7,r8,s3,s4
	umaal r7,r10,r0,r3
	umaal r8,r10,r0,r12
	umaal r9,r10,r0,lr
	umaal r10,r2,r0,r1
	add r11,r2,r11, lsr #1
	
	pop {pc}
END_FUNC(fe25519_sqr)
	
#else
	
// input: *r1=a, *r2=b
// output: r4-r11
// clobbers all other registers
// cycles: 147
START_FUNC(fe25519_mul)
	push {r2,lr}
	.cfi_def_cfa_offset 8
	.cfi_offset lr,-4
	sub sp,sp,#4*8
	.cfi_def_cfa_offset 40
	
	ldr r9,[r1],#4
	str r1,[sp,#4*7]
	
	ldm r1,{r0,r1,r3,r4} // kill r1
	ldm r2,{r5,r6,r10} // keep r2

	umull r7,r8,r5,r9 // A=r7 done
	str r7,[sp,#4+4*0]
	umull r7,r11,r5,r0
	// 11 8+7 .
	umaal r7,r8,r6,r9 // B=r7 done
	// 11+8 7 .
	str r7,[sp,#4+4*1]
	umull r7,r12,r5,r1
	// 12 11+8+7 . .
	umaal r7,r11,r6,r0
	// 12+11 8+7 . .
	umaal r7,r8,r10,r9 // C=r7 done
	// 12+11+8 7 . .
	umaal r11,r12,r5,r3
	// 12 11+8 7 . .
	umaal r8,r11,r6,r1
	// 12+11 8 7 . .
	umaal r11,r12,r5,r4
	ldr r5,[r2,#4*3]
	str r7,[sp,#4+4*2]
	// 12 11 8 . . .
	umull r7,lr,r10,r0
	// 12 lr+11 8+7 . . .
	umaal r11,lr,r6,r3
	// lr+12 11 8+7 . . .
	umaal r12,lr,r6,r4
	// lr 12 11 8+7 . . .
	umaal r7,r8,r5,r9 // r7 done
	// lr 12 11+8 7 . . .
	ldr r6,[r2,#4*4]
	str r7,[sp,#4+4*3]
	umaal r8,r11,r10,r1
	// lr 12+11 8 . . . .
	umaal r11,r12,r10,r3
	// lr+12 11 8 . . . .
	umaal r12,lr,r10,r4
	// lr 12 11 8 . . . .
	umull r7,r10,r5,r0
	// lr 12 11+10 8+7 . . . .
	umaal r10,r11,r5,r1
	umaal r11,r12,r5,r3
	umaal r12,lr,r5,r4

	// lr 12 11 10 8+7 . . . .

	umaal r7,r8,r6,r9 // r7 done
	umaal r8,r10,r6,r0
	umaal r10,r11,r6,r1
	umaal r11,r12,r6,r3
	umaal r12,lr,r6,r4

	// lr 12 11 10 8

	str r7,[sp,#4+4*4]
	ldr r5,[r2,#4*0]
	ldr r7,[sp,#4*7]
	ldr r6,[r2,#4*5]
	ldr.w r7,[r7,#4*5-4]
	umull r7,r5,r5,r7

	// lr 12 11 10+5 8+7

	umaal r7,r8,r6,r9
	umaal r5,r10,r6,r0
	umaal r10,r11,r6,r1
	umaal r11,r12,r6,r3
	umaal r12,lr,r6,r4

	// lr 12 11 10 8+5

	str r7,[sp,#4+4*5]

	ldr r7,[r2,#4*6]
	ldr.w r6,[r2,#4*7]
	umaal r5,r8,r7,r9
	umaal r8,r10,r7,r0
	umaal r10,r11,r7,r1
	umaal r11,r12,r7,r3
	umaal r12,lr,r7,r4

	// lr 12 11 10 8 5

	umull r7,r9,r6,r9
	umaal r9,r10,r6,r0
	umaal r10,r11,r6,r1
	umaal r11,r12,r6,r3
	umaal r12,lr,r6,r4

	// lr 12 11 10 9 8+7 5

	ldr r0,[sp,#4*7]
	ldr r1,[r2]
	ldr r3,[r2,#4]

	ldr r4,[r0,#4*6-4]
	ldr r2,[r0,#4*7-4]
	ldr r0,[r0,#4*5-4]
	umaal r7,r8,r1,r2
	umull r6,r1,r1,r4

	// lr 12 11 10 9+8 7+4 6+5

	umaal r6,r5,r3,r0
	str r6,[sp,#4+4*6]
	ldr r6,[sp,#4*8]
	umaal r1,r7,r3,r4
	umaal r8,r9,r3,r2

	ldr r3,[r6,#4*2]
	umaal r5,r1,r3,r0
	str r5,[sp]
	umaal r7,r8,r3,r4
	umaal r9,r10,r3,r2

	ldr r3,[r6,#4*3]
	ldr r5,[r6,#4*4]
	umaal r1,r7,r3,r0
	umaal r8,r9,r3,r4
	umaal r10,r11,r3,r2

	umaal r7,r8,r5,r0
	umaal r9,r10,r5,r4
	umaal r11,r12,r5,r2

	ldr.w r3,[r6,#4*5]
	ldr r5,[r6,#4*6]
	ldr r6,[r6,#4*7]
	umaal r8,r9,r3,r0
	umaal r10,r11,r3,r4
	umaal r12,lr,r3,r2

	umaal r9,r10,r5,r0
	umaal r11,r12,r5,r4
	umaal r12,lr,r5,r2

	umaal r10,r11,r6,r0
	umaal r12,r11,r6,r4
	umaal lr,r11,r6,r2
	
	// 11 lr 12 10 9 8 7 1 stack*8

	//now reduce
	pop {r2,r4,r5,r6}
	.cfi_def_cfa_offset 24

	movs r0,#38<<1
	mov r3,r2
	umaal r2,r3,r0,r11
	movs.w r0,#19
	mov r11,#38
	muls r0,r3,r0

	umaal r4,r0,r11,r1
	umaal r5,r0,r11,r7
	umaal r6,r0,r11,r8
	pop {r7,r8}
	.cfi_def_cfa_offset 16
	umaal r7,r0,r11,r9
	umaal r8,r0,r11,r10
	pop {r9,r10}
	.cfi_def_cfa_offset 8
	umaal r9,r0,r11,r12
	umaal r10,r0,r11,lr
	pop {r1,lr}
	.cfi_def_cfa_offset 0
	.cfi_restore lr
	add r11,r0,r2, lsr #1
	
	bx lr
END_FUNC(fe25519_mul)

	.balign 4
// input/result in (r4-r11)
// clobbers all other registers
// cycles: 91
START_FUNC(fe25519_sqr)
	str lr,[sp,#-4]!
	.cfi_def_cfa_offset 4
	.cfi_offset lr,-4
fe25519_sqr_nopush:
	sub sp,#20
	.cfi_def_cfa_offset 24
	
#define q0 r4
#define q1 r5
#define q2 r6
#define q3 r7
#define q4 r8
#define q5 r9
#define q6 r10
#define q7 r11
	movs r0,#0

	umull r1,r2,q0,q0
	and r12,q1,q0, asr #31
	adds q0,q0,q0
	str r1,[sp,#4*0]
	umull r1,r3,r0,r0 // set to 0
	umaal r1,r2,q0,q1
	umaal r2,r12,q0,q2
	umaal r2,r3,q1,q1
	str r1,[sp,#4*1]
	str r2,[sp,#4*2]
	and lr,q2,q1, asr #31
	adcs.w q1,q1,q1
	umull r1,r2,r0,r0 // set to 0
	umaal r3,r12,q0,q3
	umaal r1,r3,q1,q2
	umaal r3,r12,q0,q4
	umaal r3,lr,q1,q3
	umaal r2,r3,q2,q2
	str r1,[sp,#4*3]
	str r2,[sp,#4*4]
	umaal r3,r12,q0,q5
	umaal r3,lr,q1,q4
	umaal r12,lr,q0,q6
	and r2,q3,q2, asr #31
	adcs q2,q2,q2
	umaal r2,r12,q1,q5
	umaal r12,lr,q0,q7 // q0 dead
	umull r1,r4,r0,r0 // set to 0
	umaal r1,r3,q2,q3               // r1 (r9) done
	umaal r2,r3,q2,q4
	umaal r3,r12,q1,q6
	umaal r12,lr,q1,q7 // q1 dead
	umaal r2,r4,q3,q3               // r2 done
	umaal r0,r3,q2,q5
	umaal r3,r12,q2,q6
	umaal r12,lr,q2,q7 // q2 dead
	and r6,q4,q3, asr #31
	adcs q3,q3,q3
	umaal r4,r0,q3,q4               // r4 done
	umaal r0,r3,q3,q5
	umaal r3,r12,q3,q6
	umaal r12,lr,q3,q7 // q3 dead
	umaal r6,r0,q4,q4               // r6 done
	adcs r5,q4,q4
	umaal r0,r3,r5,q5               // r0 done
	umaal r3,r12,r5,q6
	umaal r12,lr,r5,q7 // r5 dead
	and r8,q5,q4, asr #31
	umaal r8,r3,q5,q5               // r8 done
	and r5,q6,q5, asr #31
	adcs q5,q5,q5
	umaal r3,r12,q5,q6              // r3 done
	umaal r12,lr,q5,q7 // q5 dead
	umaal r12,r5,q6,q6              // r12 done
	mov r9,r1
	and r1,q7,q6, asr #31
	adcs q6,q6,q6
	umaal lr,r5,q6,q7               // lr done
	umaal r1,r5,q7,q7               // r1, r5 done

	// msb -> lsb: r5 r1 lr r12 r3 r8 r0 r6 r4 r2 r9 sp[4] sp[3] sp[2] sp[1] sp[0]
	// now reduce!

	movs r7,#38*2
	mov r11,r4
	umaal r11,r4,r7,r5
	movs r5,#19
	mov r7,r0
	movs r0,#38
	muls r4,r5,r4

	pop {r5}
	.cfi_def_cfa_offset 20
	umaal r4,r5,r0,r6
	pop {r6,r10}
	.cfi_def_cfa_offset 12
	umaal r5,r6,r0,r7
	umaal r6,r10,r0,r8
	pop {r7,r8}
	.cfi_def_cfa_offset 4
	umaal r7,r10,r0,r3
	umaal r8,r10,r0,r12
	umaal r9,r10,r0,lr
	umaal r10,r2,r0,r1
	pop {lr}
	.cfi_def_cfa_offset 0
	.cfi_restore lr
	add r11,r2,r11, lsr #1
	
	bx lr
END_FUNC(fe25519_sqr)
#endif

#ifdef has_d_cache
// in: r4-r11 (first operand), *r2 (second operand), r1 (0 = first operand, 1 = second operand)
// out: r4-r11 = the operand squared
// clobbers all other registers
START_FUNC(fe25519_csel_sqr)
	push {lr}
	.cfi_def_cfa_offset 4
	.cfi_offset lr,-4
	subs r0,r1,#1
	ldm r2!,{r3,r12,lr}
	ands r4,r4,r0
	mla r4,r1,r3,r4
	ands r5,r5,r0
	mla r5,r1,r12,r5
	ands r6,r6,r0
	mla r6,r1,lr,r6
	ldm r2!,{r3,r12,lr}
	ands r7,r7,r0
	mla r7,r1,r3,r7
	and r8,r8,r0
	mla r8,r1,r12,r8
	and r9,r9,r0
	mla r9,r1,lr,r9
	ldm r2,{r2,r3}
	and r10,r10,r0
	mla r10,r1,r2,r10
	and r11,r11,r0
	mla r11,r1,r3,r11
	b fe25519_sqr_nopush
END_FUNC(fe25519_csel_sqr)
#endif

	.balign 4
// in: *r0 = result, *r1 = scalar, *r2 = basepoint (all pointers may be unaligned)
// cycles: 441 116
START_GLOBAL_FUNC(curve25519_scalarmult)
	push {r0,r4-r11,lr}
	.cfi_def_cfa_offset 40
	.cfi_offset r4,-36
	.cfi_offset r5,-32
	.cfi_offset r6,-28
	.cfi_offset r7,-24
	.cfi_offset r8,-20
	.cfi_offset r9,-16
	.cfi_offset r10,-12
	.cfi_offset r11,-8
	.cfi_offset lr,-4
	
	sub sp,sp,#268
	.cfi_def_cfa_offset 308
	
	movs r3,#1
	movs r4,#0
	
	add r5,sp,#32*2
	add r0,sp,#224
	stm r5!,{r3} // x2 = 1
0:
	stm r5!,{r4}
	cmp r5,r0
	blt 0b
	str r3,[sp,#32*5] // z3 = 1
	
	bl curve25519_copy // scalar
	
	mov r1,r2
	add r0,sp,#32*6
	bl curve25519_copy // x1
	add r1,sp,#32*6
	add r0,sp,#32*4
	ldr r2,[r1,#28]
	bic r2,r2,#0x80000000 // clear most significant bit in basepoint
	str r2,[r1,#28]
	bl curve25519_copy // x3
	
	ldr r0,[sp,#224]
	ldr r1,[sp,#252]
	bic r0,r0,#7            // clamp scalar by setting lowest three bits to zero
	orr r1,r1,#0x40000000   // highest bit shall also be set
	str r0,[sp,#224]
	str r1,[sp,#252]
	
	movs r0,#254 // bitpos
	movs r1,#0 // lastbit
	
	// a b x2 z2 x3 z3 x1 scalar bitpos lastbit csel
	// 0                  224    256    260     264
	
0:
	add r2,sp,#224
	lsrs r3,r0,#5
	ldr r4,[r2,r3, lsl #2]
	str r0,[sp,#256]
	and r5,r0,#0x1f
	lsrs r4,r4,r5
	and r4,r4,#1
	eors r1,r1,r4
#ifndef has_d_cache
	add r1,sp,r1, lsl #6
#endif
	str r4,[sp,#260]
	str r1,[sp,#264]
	
	add r1,sp,#32*2
	add r2,sp,#32*3
	bl fe25519_add	      // a = x2 + z2
	stm sp,{r4-r11}
	add r1,sp,#32*2
	add r2,sp,#32*3
	bl fe25519_sub	      // b = x2 - z2
	add r0,sp,#32*1
	add r1,sp,#32*4
	add r2,sp,#32*5
	stm r0,{r4-r11}
	bl fe25519_add        // x2 = x3 + z3
	add r0,sp,#32*2
	add r1,sp,#32*4
	add r2,sp,#32*5
	stm r0,{r4-r11}
	bl fe25519_sub        // z2 = x3 - z3
	add r0,sp,#32*3
	add r1,sp,#32*3
	add r2,sp,#32*0
	stm r0,{r4-r11}
	bl fe25519_mul        // z3 = z2 * a
	add r0,sp,#32*5
	ldr r1,[sp,#264]
	stm r0,{r4-r11}
#ifndef has_d_cache       // a = f^2 (f is a or x2)
	ldm r1,{r4-r11}
	bl fe25519_sqr
#else
	add r2,sp,#32*2
	ldm sp,{r4-r11}
	bl fe25519_csel_sqr
#endif
	stm sp,{r4-r11}
	add r1,sp,#32*2
	add r2,sp,#32*1
	bl fe25519_mul        // x3 = x2 * b
	add r0,sp,#32*4
	ldr r1,[sp,#264]
#ifndef has_d_cache       // x2 = g^2 (g is b or z2)
	adds r1,r1,#32*1
	stm r0,{r4-r11}
	ldm r1,{r4-r11}
	bl fe25519_sqr
#else
	add r2,sp,#32*1
	stm r0,{r4-r11}
	ldm r2,{r4-r11}
	add r2,sp,#32*3
	bl fe25519_csel_sqr
#endif
	add r0,sp,#32*2
	add r1,sp,#32*5
	add r2,sp,#32*4
	stm r0,{r4-r11}
	bl fe25519_sub        // _ = z3 - x3
	bl fe25519_sqr        // z2 = _^2
	add r0,sp,#32*3
	add r1,sp,#32*5
	add r2,sp,#32*4
	stm r0,{r4-r11}
	bl fe25519_add        // _ = z3 + x3
	bl fe25519_sqr        // x3 = _^2
	add r0,sp,#32*4
	add r1,sp,#32*6
	add r2,sp,#32*3
	stm r0,{r4-r11}
	bl fe25519_mul        // z3 = x1 * z2
	add r0,sp,#32*5
	add r1,sp,#32*0
	add r2,sp,#32*2
	stm r0,{r4-r11}
	bl fe25519_sub        // b = a - x2
	
	add r2,sp,#32*1
	mov lr,#19
	ldr r3,[sp,#32*2+28]
	ldr r1,=121666        // z2 = x2 + 121666 * b
	stm r2,{r4-r11}
	add r12,r1,r1
	mov r2,r3
	umaal r3,r2,r12,r11
	add r0,sp,#32*2
	mul lr,lr,r2
	ldm r0!,{r2,r11,r12}
	umaal r2,lr,r1,r4
	add r4,sp,#32*3
	umaal r11,lr,r1,r5
	umaal r12,lr,r1,r6
	stm r4!,{r2,r11,r12}
	ldm r0,{r0,r2,r5,r6}
	umaal r0,lr,r1,r7
	umaal r2,lr,r1,r8
	umaal r5,lr,r1,r9
	umaal r6,lr,r1,r10
	add r1,sp,#32*0
	add lr,lr,r3, lsr #1
	stm r4!,{r0,r2,r5,r6,lr}
	
	add r2,sp,#32*2
	bl fe25519_mul        // x2 = a * x2
	add r0,sp,#32*2
	add r1,sp,#32*1
	add r2,sp,#32*3
	stm r0,{r4-r11}
	bl fe25519_mul        // z2 = b * z2
	add r2,sp,#32*3
	ldr r0,[sp,#256]
	ldr r1,[sp,#260]
	stm r2,{r4-r11}
	subs r0,r0,#1
	bpl 0b
	
	// now calculate x*z^-1
	
	add sp,sp,#32
	.cfi_def_cfa_offset 276
	
	bl fe25519_sqr              // 2
	add r0,sp,#32*5
	stm r0,{r4-r11}
	bl fe25519_sqr              // 4
	bl fe25519_sqr              // 8
	stm sp,{r4-r11}
	add r1,sp,#32*2
	mov r2,sp
	bl fe25519_mul              // 9
	add r0,sp,#32*3
	add r1,sp,#32*5
	add r2,sp,#32*3
	stm r0,{r4-r11}
	bl fe25519_mul              // 11
	add r0,sp,#32*5
	add r1,sp,#32*3
	movs r2,#1
	stm r0,{r4-r11}
	
	adr r3,fe25519_invsequence
0:
	ldrh r0,[r3],#2
	cbz r0,2f
	and r1,r0,#0xe0
	add r1,r1,sp // out pointer
	str r3,[sp,#192] // current invsequence position
	stm r1,{r4-r11}
	lsrs r2,r0,#8 // loop count
1:
	str r2,[sp,#196]
	bl fe25519_sqr
	ldr r2,[sp,#196]
	subs r2,r2,#1
	bne 1b
	
	ldr r3,[sp,#192]
	ldrh r1,[r3,#-2]
	lsls r1,r1,#29
	add r1,sp,r1, lsr #24
	stm sp,{r4-r11}
	mov r2,sp
	bl fe25519_mul
	ldr r3,[sp,#192]
	b 0b
2:
	stm sp,{r4-r11}
	mov r1,sp
	add r2,sp,#32*1
	bl fe25519_mul
	
	// the reduction below works only when 0 <= x < 2^256-38
	
	movs r3,#0
	movs r2,#19
	
	adds r4,r4,r2
	adcs r5,r5,r3
	adcs r6,r6,r3
	adcs r7,r7,r3
	adcs r8,r8,r3
	adcs r9,r9,r3
	adcs r10,r10,r3
	adc r11,r11,r3
	
	bic r2,r2,r11, asr #31
	bic r11,r11,#0x80000000
	
	subs r4,r4,r2
	sbcs r5,r5,r3
	sbcs r6,r6,r3
	sbcs r7,r7,r3
	sbcs r8,r8,r3
	sbcs r9,r9,r3
	sbcs r10,r10,r3
	sbcs r11,r11,r3
	
	push {r4-r11}
	.cfi_def_cfa_offset 308
	
	ldr r0,[sp,#268]
	mov r1,sp
	bl curve25519_copy
	
	add sp,sp,#272
	.cfi_def_cfa_offset 36
	pop {r4-r11,pc}

	.balign 4
fe25519_invsequence:
	// each operation calculates c = (a^2)^n * b, where a is the last operation's result
	// out address (for previous operation), in address for mul operand, sqr count
	.short (5<<5) | 3 | (1<<8)   // 2^5 - 2^0 = 31
	.short (3<<5) | 3 | (5<<8)   // 2^10 - 2^0
	.short (3<<5) | 3 | (10<<8)  // 2^20 - 2^0
	.short (4<<5) | 4 | (20<<8)  // 2^40 - 2^0
	.short (4<<5) | 3 | (10<<8)  // 2^50 - 2^0
	.short (3<<5) | 3 | (50<<8)  // 2^100 - 2^0
	.short (4<<5) | 4 | (100<<8) // 2^200 - 2^0
	.short (4<<5) | 3 | (50<<8)  // 2^250 - 2^0
	.short (4<<5) | 5 | (5<<8)   // 2^255 - 2^5 + 11 = (2^255 - 19) - 2
	.short 0                     // exit
END_FUNC(curve25519_scalarmult)

// input: *r1
// output: *r0
// performs unaligned copy of 32 bytes
START_FUNC(curve25519_copy)
	movs r4,#8
0:
	ldr r3,[r1],#4
	str r3,[r0],#4
	subs r4,r4,#1
	bgt 0b
	bx lr
END_FUNC(curve25519_copy)

	.section .note.GNU-stack,"",%progbits
